{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Notebook for model training\n",
    "\n",
    "### Training Flow\n",
    "\n",
    "Load Training Data:\n",
    "- Load set of input data and label files\n",
    "- Prefilter positive set only to rule out heavily-masked patches\n",
    "- Create train/test set\n",
    "- Define augmentation parameters\n",
    "\n",
    "Train Model:\n",
    "- Define model architecture\n",
    "- Compile model\n",
    "- Train and evaluate model\n",
    "- Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 13:59:08.530124: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-27 13:59:08.551867: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-27 13:59:08.551890: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-27 13:59:08.552492: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-27 13:59:08.556442: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-27 13:59:09.285533: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Conv2D\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "parent_dir = os.path.split(os.getcwd())[0]\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "from scripts import dl_utils\n",
    "from scripts import viz_tools\n",
    "from scripts import models\n",
    "\n",
    "import gc\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# export TF_GPU_ALLOCATOR=cuda_malloc_async\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a46688369b4e0db93001dc8142c4e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4549 samples loaded\n",
      "1891 positive samples\n",
      "2658 negative samples\n"
     ]
    }
   ],
   "source": [
    "resolution = 48\n",
    "\n",
    "data_files = [\n",
    "    \"amazonas_2020_thresh_0.5_2_negatives_2019-01-01_2020-01-01_patch_arrays.pkl\",\n",
    "    \"amazonas_2020_thresh_0.8_sumbsample_3_positives_2019-01-01_2020-01-01_patch_arrays.pkl\",\n",
    "    \"MinesPos2018-2020Sentinel_points_2019-01-01_2020-01-01_patch_arrays.pkl\",\n",
    "    \"bolivar_2020_thresh_0.8_sumbsample_5_positives_2019-01-01_2020-01-01_patch_arrays.pkl\",\n",
    "    \"full_amazon_v9_negatives_2019-01-01_2020-01-01_patch_arrays.pkl\",\n",
    "    \"v2.4_amazon_positives_2019-01-01_2020-01-01_patch_arrays.pkl\",\n",
    "    \"v2.6_amazon_thresh_0.8_negatives_2019-01-01_2020-01-01_patch_arrays.pkl\",\n",
    "    \"v2.0_bolivar_negatives_2019-01-01_2020-01-01_patch_arrays.pkl\",\n",
    "    \"riverbank_negatives_2019-01-01_2020-01-01_patch_arrays.pkl\",\n",
    "    \"bolivar_2020_thresh_0.8_1_negatives_2019-01-01_2020-01-01_patch_arrays.pkl\",\n",
    "    \"v2.1.1_bolivar_negatives_2019-01-01_2020-01-01_patch_arrays.pkl\",\n",
    "    \"v2.1_bolivar_positives_2019-01-01_2020-01-01_patch_arrays.pkl\",\n",
    "    \"v2.4_amazonas_negatives_2019-01-01_2020-01-01_patch_arrays.pkl\",\n",
    "    \"v2.4_amazon_negatives_2019-01-01_2020-01-01_patch_arrays.pkl\",\n",
    "    \"v2.6_amazon_negatives_2019-01-01_2020-01-01_patch_arrays.pkl\",\n",
    "    \"v2.6_amazon_negatives_v2_2019-01-01_2020-01-01_patch_arrays.pkl\",\n",
    "    \"v3.1_2023_negatives_2023-01-01_2024-01-01_patch_arrays.pkl\",\n",
    "    \"amazon_all_48px_v3.1_2023_positives_0.999_2023-01-01_2024-01-01_patch_arrays.pkl\",\n",
    "    \"v3.2_2023_negatives_2023-01-01_2024-01-01_patch_arrays.pkl\",\n",
    "    \"v3.3_2023_positives_2023-01-01_2024-01-01_patch_arrays.pkl\",\n",
    "    \"v3.4_2023_negatives_2023-01-01_2024-01-01_patch_arrays.pkl\",\n",
    "    \"v3.5_2023_negatives_2023-01-01_2024-01-01_patch_arrays.pkl\",\n",
    "    \"v3.6_2023_positives_2023-01-01_2024-01-01_patch_arrays.pkl\"\n",
    "    \n",
    "    ]\n",
    "\n",
    "label_files = [\n",
    "    \"amazonas_2020_thresh_0.5_2_negatives_2019-01-01_2020-01-01_patch_array_labels.pkl\",\n",
    "    \"amazonas_2020_thresh_0.8_sumbsample_3_positives_2019-01-01_2020-01-01_patch_array_labels.pkl\",\n",
    "    \"MinesPos2018-2020Sentinel_points_2019-01-01_2020-01-01_patch_array_labels.pkl\",\n",
    "    \"bolivar_2020_thresh_0.8_sumbsample_5_positives_2019-01-01_2020-01-01_patch_array_labels.pkl\",\n",
    "    \"full_amazon_v9_negatives_2019-01-01_2020-01-01_patch_array_labels.pkl\",\n",
    "    \"v2.4_amazon_positives_2019-01-01_2020-01-01_patch_array_labels.pkl\",\n",
    "    \"v2.6_amazon_thresh_0.8_negatives_2019-01-01_2020-01-01_patch_array_labels.pkl\",\n",
    "    \"v2.0_bolivar_negatives_2019-01-01_2020-01-01_patch_array_labels.pkl\",\n",
    "    \"riverbank_negatives_2019-01-01_2020-01-01_patch_array_labels.pkl\",\n",
    "    \"bolivar_2020_thresh_0.8_1_negatives_2019-01-01_2020-01-01_patch_array_labels.pkl\",\n",
    "    \"v2.1.1_bolivar_negatives_2019-01-01_2020-01-01_patch_array_labels.pkl\",\n",
    "    \"v2.1_bolivar_positives_2019-01-01_2020-01-01_patch_array_labels.pkl\",\n",
    "    \"v2.4_amazonas_negatives_2019-01-01_2020-01-01_patch_array_labels.pkl\",\n",
    "    \"v2.4_amazon_negatives_2019-01-01_2020-01-01_patch_array_labels.pkl\",\n",
    "    \"v2.6_amazon_negatives_2019-01-01_2020-01-01_patch_array_labels.pkl\",\n",
    "    \"v2.6_amazon_negatives_v2_2019-01-01_2020-01-01_patch_array_labels.pkl\",\n",
    "    \"v3.1_2023_negatives_2023-01-01_2024-01-01_patch_array_labels.pkl\",\n",
    "    \"amazon_all_48px_v3.1_2023_positives_0.999_2023-01-01_2024-01-01_patch_array_labels.pkl\",\n",
    "    \"v3.2_2023_negatives_2023-01-01_2024-01-01_patch_array_labels.pkl\",\n",
    "    \"v3.3_2023_positives_2023-01-01_2024-01-01_patch_array_labels.pkl\",\n",
    "    \"v3.4_2023_negatives_2023-01-01_2024-01-01_patch_array_labels.pkl\",\n",
    "    \"v3.5_2023_negatives_2023-01-01_2024-01-01_patch_array_labels.pkl\",\n",
    "    \"v3.6_2023_positives_2023-01-01_2024-01-01_patch_array_labels.pkl\"\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "patches = []\n",
    "labels = []\n",
    "\n",
    "data_dir = os.path.join('..', 'data', 'training_data', f\"{resolution}_px\")\n",
    "\n",
    "for data, label in tqdm(zip(data_files, label_files), total=len(data_files)):\n",
    "    with open(os.path.join(data_dir, data), 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        for elem in data:\n",
    "            #patch = dl_utils.pad_patch(elem, resolution)\n",
    "            patches.append(elem)\n",
    "    with open(os.path.join(data_dir, label), 'rb') as f:\n",
    "        label = pickle.load(f)\n",
    "        labels = np.concatenate((labels, label))\n",
    "\n",
    "patches = np.array(patches)\n",
    "negative_patches = patches[labels == 0]\n",
    "positive_patches = patches[labels == 1]\n",
    "\n",
    "print(len(patches), \"samples loaded\")\n",
    "print(sum(labels == 1), \"positive samples\")\n",
    "print(sum(labels == 0), \"negative samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 64\n",
    "indices = np.random.randint(0, len(patches), num_samples)\n",
    "# viz_tools.plot_image_grid(patches[indices], labels=[int(label) for label in labels[indices]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_black(data, mask_limit=0.1, return_rejects=False):\n",
    "    masked_fraction = np.array([np.sum(np.mean(patch, axis=-1) < 10) / np.size(np.mean(patch, axis=-1)) for patch in data])\n",
    "    filtered_data = data[masked_fraction < mask_limit]\n",
    "    print(f\"{len(filtered_data) / len(data) :.1%} of data below brightness limit\")\n",
    "    if return_rejects:\n",
    "        rejected_data = data[masked_fraction >= mask_limit]\n",
    "        return filtered_data, rejected_data\n",
    "    else:\n",
    "        return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.9% of data below brightness limit\n"
     ]
    }
   ],
   "source": [
    "# Filter positive pixels that are masked beyond a threshold. Don't want to give positive examples of cloud-masked patches\n",
    "filtered_positives, positive_rejects = filter_black(positive_patches, mask_limit = 0.1, return_rejects=True)\n",
    "if len(positive_rejects) > 0:\n",
    "    num_samples = 16**2\n",
    "    if len(positive_rejects) < num_samples:\n",
    "        num_samples = len(positive_rejects)\n",
    "    indices = np.random.randint(0, len(positive_rejects), num_samples)\n",
    "    # viz_tools.plot_numpy_grid(positive_rejects[indices,:,:,3:0:-1] / 3000)\n",
    "    # plt.title(\"Positive Masked Rejects\")\n",
    "    # plt.show()\n",
    "\n",
    "num_samples = 25 ** 2\n",
    "indices = np.random.randint(0, len(filtered_positives), num_samples)\n",
    "# fig = viz_tools.plot_numpy_grid(filtered_positives[indices,:,:,3:0:-1] / 3000)\n",
    "# plt.title('Positive Filtered Samples')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% of data below brightness limit\n"
     ]
    }
   ],
   "source": [
    "filtered_negatives, negative_rejects = filter_black(negative_patches, mask_limit = 0.6, return_rejects=True)\n",
    "if len(negative_rejects) > 0:\n",
    "    num_samples = 16**2\n",
    "    if len(positive_rejects) < num_samples:\n",
    "        num_samples = len(positive_rejects)\n",
    "    indices = np.random.randint(0, len(negative_rejects), num_samples)\n",
    "    # viz_tools.plot_numpy_grid(negative_rejects[indices,:,:,3:0:-1] / 3000)\n",
    "    # plt.title(\"Negative Mask Rejects\")\n",
    "    # plt.show()\n",
    "\n",
    "num_samples = 25 ** 2\n",
    "indices = np.random.randint(0, len(filtered_negatives), num_samples)\n",
    "# fig = viz_tools.plot_numpy_grid(filtered_negatives[indices,:,:,3:0:-1] / 3000)\n",
    "# plt.title('Negative Filtered Samples')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for RGBIR, x = normalize(np.copy(images[:,:,:,[1,2,3,8]]))\n",
    "x = np.concatenate((filtered_negatives, filtered_positives))\n",
    "y = np.concatenate((np.zeros(len(filtered_negatives)), np.ones(len(filtered_positives))))\n",
    "x, y = shuffle(x, y, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_parameters = {\n",
    "    'featurewise_center': False,\n",
    "    'rotation_range': 360,\n",
    "    'width_shift_range': [0.9, 1.1],\n",
    "    'height_shift_range': [0.9, 1.1],\n",
    "    'shear_range': 10,\n",
    "    'zoom_range': [0.9, 1.1],\n",
    "    'vertical_flip': True,\n",
    "    'horizontal_flip': True,\n",
    "    # Fill options: \"constant\", \"nearest\", \"reflect\" or \"wrap\"\n",
    "    'fill_mode': 'reflect'\n",
    "}\n",
    "\n",
    "datagen = ImageDataGenerator(**augmentation_parameters)\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(12,12), facecolor=(1,1,1), dpi=150)\n",
    "# aug_img, aug_labels = datagen.flow(x, y, batch_size=64).next()\n",
    "# viz_tools.plot_image_grid(aug_img, labels=[int(l) for l in aug_labels], norm=True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm = np.clip(np.array(x.astype(\"float32\") / 10000), 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partimos el 10% del dataset total para test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(y)\n",
    "test_partition = int(dataset_size*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de clases positivas en el dataset de training es de 1705, número de clases negativas 2389\n"
     ]
    }
   ],
   "source": [
    "num_positive_samples_train = sum(y[test_partition:]==1)\n",
    "num_negative_samples_train = sum(y[test_partition:]==0)\n",
    "\n",
    "print(f\"Número de clases positivas en el dataset de training es de {num_positive_samples_train}, número de clases negativas {num_negative_samples_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos estos samples del dataset de validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_partition_x = x_norm[:test_partition]\n",
    "test_partition_y = y[:test_partition]\n",
    "\n",
    "x_norm = x_norm[test_partition:]\n",
    "y = y[test_partition:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Hiperparámetros ejecuciones originales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Preparación de hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Balanceo de clases para el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "El número de valores para la clase no mina es de 0.8568438677270824 y para la clase mina 1.2005865102639297\n",
      "\n",
      "Ponemos peso a cada clase para tenerlo en cuenta en el entrenamiento {0: 0.8568438677270824, 1: 1.2005865102639297}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Suponiendo que 'y_train' es un array de las etiquetas de entrenamiento\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y),\n",
    "    y=y\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print(f\"\\nEl número de valores para la clase no mina es de {class_weight_dict[0]} y para la clase mina {class_weight_dict[1]}\")\n",
    "print(f\"\\nPonemos peso a cada clase para tenerlo en cuenta en el entrenamiento {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Clases no balanceadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a weighted loss\n",
    "class_weight = {0: 1, 1: 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diccionario con todos los hiperparámetros editables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Hiperparámetros para pruebas: early stopping = 7 y LR descendiente = 4 con test size del 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "########################################## CLASES BALANCEADAS ###########################################\n",
    "#########################################################################################################\n",
    "                                                                                                        #\n",
    "# reduce_lr en callbacks                                                                                #                            \n",
    "hiper_params_test1_20 = {\"class_weight\" : class_weight_dict, #\n",
    "                      \"batch_size\" : 32,                                                                #\n",
    "                      \"epochs\" : 160,                                                                   #\n",
    "                      \"define_callbacks\" : [\"reduce_lr\", \"early_stopping\"],         #\n",
    "                      \"patience_early_stopping\" : 7,                                                   #\n",
    "                      \"patience_reduce_lr\" : 4,                                                         #\n",
    "                      \"test_size\" : 0.2                                                                 #\n",
    "                     }                                                                                  #\n",
    "                                                                                                        #\n",
    "# SIN reduce_lr en callbacks                                                                            #\n",
    "hiper_params_test2_20 = {\"class_weight\" : class_weight_dict, #\n",
    "                      \"batch_size\" : 32,                                                                #\n",
    "                      \"epochs\" : 160,                                                                   #\n",
    "                      \"define_callbacks\" : [\"early_stopping\"],                      #\n",
    "                      \"patience_early_stopping\" : 7,                                                   #\n",
    "                      \"test_size\" : 0.2                                                                 #\n",
    "                     }                                                                                  #\n",
    "#########################################################################################################\n",
    "\n",
    "#########################################################################################################\n",
    "####################################### CLASES NO BALANCEADAS ###########################################\n",
    "#########################################################################################################\n",
    "                                                                                                        #\n",
    "# Reduce_lr en callbacks                                                                                #\n",
    "hiper_params_test3_20 = {\"class_weight\" : {0: 1, 1: 1},                                                 #\n",
    "                      \"batch_size\" : 32,                                                                #\n",
    "                      \"epochs\" : 160,                                                                   #\n",
    "                      \"define_callbacks\" : [\"reduce_lr\", \"early_stopping\"],         #\n",
    "                      \"patience_early_stopping\" : 7,                                                   #\n",
    "                      \"patience_reduce_lr\" : 4,                                                         #\n",
    "                      \"test_size\" : 0.2                                                                 #\n",
    "                     }                                                                                  #\n",
    "#SIN Reduce_lr en callbacks                                                                             #\n",
    "hiper_params_test4_20 = {\"class_weight\" : {0: 1, 1: 1},                                                 #\n",
    "                      \"batch_size\" : 32,                                                                #\n",
    "                      \"epochs\" : 160,                                                                   #\n",
    "                      \"define_callbacks\" : [\"early_stopping\"],                      #\n",
    "                      \"patience_early_stopping\" : 7,                                                   #\n",
    "                      \"test_size\" : 0.2                                                                 #\n",
    "                     }                                                                                  #\n",
    "#########################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Hiperparámetros para pruebas: early stopping = 7 y LR descendiente = 4 con test size del 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "###################################### CLASES BALANCEADAS ###############################################\n",
    "#########################################################################################################\n",
    "                                                                                                        #\n",
    "# Reduce_lr en callbacks                                                                                #\n",
    "hiper_params_test1_30 = {\"class_weight\" : class_weight_dict, #\n",
    "                      \"batch_size\" : 16,                                                                #\n",
    "                      \"epochs\" : 160,                                                                   #\n",
    "                      \"define_callbacks\" : [\"reduce_lr\", \"early_stopping\"],         #\n",
    "                      \"patience_early_stopping\" : 7,                                                   #\n",
    "                      \"patience_reduce_lr\" : 4,                                                         #\n",
    "                      \"test_size\" : 0.3                                                                 #\n",
    "                     }                                                                                  #         \n",
    "                                                                                                        #\n",
    "# SIN reduce_lr en callbacks                                                                            #   \n",
    "hiper_params_test2_30 = {\"class_weight\" : class_weight_dict, #\n",
    "                      \"batch_size\" : 16,                                                                #\n",
    "                      \"epochs\" : 160,                                                                   #\n",
    "                      \"define_callbacks\" : [\"early_stopping\"],                      #\n",
    "                      \"patience_early_stopping\" : 7,                                                   #\n",
    "                      \"test_size\" : 0.3                                                                 #\n",
    "                     }                                                                                  #\n",
    "#########################################################################################################\n",
    "\n",
    "######################################################################################################\n",
    "###################################### CLASES NO BALANCEADAS #########################################\n",
    "######################################################################################################\n",
    "                                                                                                     #\n",
    "# Reduce_lr en callbacks                                                                             #  \n",
    "hiper_params_test3_30 = {\"class_weight\" : {0: 1, 1: 1},                                              #\n",
    "                      \"batch_size\" : 16,                                                             #\n",
    "                      \"epochs\" : 160,                                                                #\n",
    "                      \"define_callbacks\" : [\"reduce_lr\", \"early_stopping\"],      #\n",
    "                      \"patience_early_stopping\" : 7,                                                #\n",
    "                      \"patience_reduce_lr\" : 4,                                                      #\n",
    "                      \"test_size\" : 0.3                                                              #\n",
    "                     }                                                                               #\n",
    "                                                                                                     #\n",
    "# SIN reduce_lr en callbacks                                                                         #      \n",
    "hiper_params_test4_30 = {\"class_weight\" : {0: 1, 1: 1},                                              #\n",
    "                      \"batch_size\" : 16,                                                             #\n",
    "                      \"epochs\" : 160,                                                                #\n",
    "                      \"define_callbacks\" : [\"early_stopping\"],                   #\n",
    "                      \"patience_early_stopping\" : 7,                                                #\n",
    "                      \"test_size\" : 0.3                                                              #\n",
    "                     }                                                                               #\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Hiperparámetros para pruebas: aumento patience en early stopping = 14 y LR descendiente = 8, split de 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "########################################## CLASES BALANCEADAS ###########################################\n",
    "#########################################################################################################\n",
    "                                                                                                        #\n",
    "# reduce_lr en callbacks                                                                                #                            \n",
    "hiper_params_test5_20 = {\"class_weight\" : class_weight_dict, #\n",
    "                      \"batch_size\" : 32,                                                                #\n",
    "                      \"epochs\" : 160,                                                                   #\n",
    "                      \"define_callbacks\" : [\"reduce_lr\", \"early_stopping\"],         #\n",
    "                      \"patience_early_stopping\" : 14,                                                   #\n",
    "                      \"patience_reduce_lr\" : 8,                                                        #\n",
    "                      \"test_size\" : 0.2                                                                 #\n",
    "                     }                                                                                  #\n",
    "                                                                                                        #\n",
    "# SIN reduce_lr en callbacks                                                                            #\n",
    "hiper_params_test6_20 = {\"class_weight\" : class_weight_dict, #\n",
    "                      \"batch_size\" : 32,                                                                #\n",
    "                      \"epochs\" : 160,                                                                   #\n",
    "                      \"define_callbacks\" : [\"early_stopping\"],                      #\n",
    "                      \"patience_early_stopping\" : 14,                                                   #\n",
    "                      \"test_size\" : 0.2                                                                 #\n",
    "                     }                                                                                  #\n",
    "#########################################################################################################\n",
    "\n",
    "#########################################################################################################\n",
    "####################################### CLASES NO BALANCEADAS ###########################################\n",
    "#########################################################################################################\n",
    "                                                                                                        #\n",
    "# Reduce_lr en callbacks                                                                                #\n",
    "hiper_params_test7_20 = {\"class_weight\" : {0: 1, 1: 1},                                                 #\n",
    "                      \"batch_size\" : 32,                                                                #\n",
    "                      \"epochs\" : 160,                                                                   #\n",
    "                      \"define_callbacks\" : [\"reduce_lr\", \"early_stopping\"],         #\n",
    "                      \"patience_early_stopping\" : 14,                                                   #\n",
    "                      \"patience_reduce_lr\" : 8,                                                        #\n",
    "                      \"test_size\" : 0.2                                                                 #\n",
    "                     }                                                                                  #\n",
    "#SIN Reduce_lr en callbacks                                                                             #\n",
    "hiper_params_test8_20 = {\"class_weight\" : {0: 1, 1: 1},                                                 #\n",
    "                      \"batch_size\" : 32,                                                                #\n",
    "                      \"epochs\" : 160,                                                                   #\n",
    "                      \"define_callbacks\" : [\"early_stopping\"],                      #\n",
    "                      \"patience_early_stopping\" : 14,                                                   #\n",
    "                      \"test_size\" : 0.2                                                                 #\n",
    "                     }                                                                                  #\n",
    "#########################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Hiperparámetros para pruebas: aumento patience en early stopping = 14 y LR descendiente = 8, split de 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "###################################### CLASES BALANCEADAS ###############################################\n",
    "#########################################################################################################\n",
    "                                                                                                        #\n",
    "# Reduce_lr en callbacks                                                                                #\n",
    "hiper_params_test5_30 = {\"class_weight\" : class_weight_dict, #\n",
    "                      \"batch_size\" : 16,                                                                #\n",
    "                      \"epochs\" : 160,                                                                   #\n",
    "                      \"define_callbacks\" : [\"reduce_lr\", \"early_stopping\"],         #\n",
    "                      \"patience_early_stopping\" : 14,                                                   #\n",
    "                      \"patience_reduce_lr\" : 8,                                                        #\n",
    "                      \"test_size\" : 0.3                                                                 #\n",
    "                     }                                                                                  #         \n",
    "                                                                                                        #\n",
    "# SIN reduce_lr en callbacks                                                                            #   \n",
    "hiper_params_test6_30 = {\"class_weight\" : class_weight_dict, #\n",
    "                      \"batch_size\" : 16,                                                                #\n",
    "                      \"epochs\" : 160,                                                                   #\n",
    "                      \"define_callbacks\" : [\"early_stopping\"],                      #\n",
    "                      \"patience_early_stopping\" : 14,                                                   #\n",
    "                      \"test_size\" : 0.3                                                                 #\n",
    "                     }                                                                                  #\n",
    "#########################################################################################################\n",
    "\n",
    "######################################################################################################\n",
    "###################################### CLASES NO BALANCEADAS #########################################\n",
    "######################################################################################################\n",
    "                                                                                                     #\n",
    "# Reduce_lr en callbacks                                                                             #  \n",
    "hiper_params_test7_30 = {\"class_weight\" : {0: 1, 1: 1},                                              #\n",
    "                      \"batch_size\" : 16,                                                             #\n",
    "                      \"epochs\" : 160,                                                                #\n",
    "                      \"define_callbacks\" : [\"reduce_lr\", \"early_stopping\"],      #\n",
    "                      \"patience_early_stopping\" : 14,                                                #\n",
    "                      \"patience_reduce_lr\" : 8,                                                     #\n",
    "                      \"test_size\" : 0.3                                                              #\n",
    "                     }                                                                               #\n",
    "                                                                                                     #\n",
    "# SIN reduce_lr en callbacks                                                                         #      \n",
    "hiper_params_test8_30 = {\"class_weight\" : {0: 1, 1: 1},                                              #\n",
    "                      \"batch_size\" : 16,                                                             #\n",
    "                      \"epochs\" : 160,                                                                #\n",
    "                      \"define_callbacks\" : [\"early_stopping\"],                   #\n",
    "                      \"patience_early_stopping\" : 14,                                                #\n",
    "                      \"test_size\" : 0.3                                                              #\n",
    "                     }                                                                               #\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Hiperparámetros para pruebas: aumento patience en early stopping = 20 y LR descendiente = 10, split de 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "########################################## CLASES BALANCEADAS ###########################################\n",
    "#########################################################################################################\n",
    "                                                                                                        #\n",
    "# reduce_lr en callbacks                                                                                #                            \n",
    "hiper_params_test9_20 = {\"class_weight\" : class_weight_dict, #\n",
    "                      \"batch_size\" : 32,                                                                #\n",
    "                      \"epochs\" : 160,                                                                   #\n",
    "                      \"define_callbacks\" : [\"reduce_lr\", \"early_stopping\"],         #\n",
    "                      \"patience_early_stopping\" : 20,                                                   #\n",
    "                      \"patience_reduce_lr\" : 10,                                                        #\n",
    "                      \"test_size\" : 0.2                                                                 #\n",
    "                     }                                                                                  #\n",
    "                                                                                                        #\n",
    "# SIN reduce_lr en callbacks                                                                            #\n",
    "hiper_params_test10_20 = {\"class_weight\" : class_weight_dict, #\n",
    "                      \"batch_size\" : 32,                                                                #\n",
    "                      \"epochs\" : 160,                                                                   #\n",
    "                      \"define_callbacks\" : [\"early_stopping\"],                      #\n",
    "                      \"patience_early_stopping\" : 20,                                                   #\n",
    "                      \"test_size\" : 0.2                                                                 #\n",
    "                     }                                                                                  #\n",
    "#########################################################################################################\n",
    "\n",
    "#########################################################################################################\n",
    "####################################### CLASES NO BALANCEADAS ###########################################\n",
    "#########################################################################################################\n",
    "                                                                                                        #\n",
    "# Reduce_lr en callbacks                                                                                #\n",
    "hiper_params_test11_20 = {\"class_weight\" : {0: 1, 1: 1},                                                 #\n",
    "                      \"batch_size\" : 32,                                                                #\n",
    "                      \"epochs\" : 160,                                                                   #\n",
    "                      \"define_callbacks\" : [\"reduce_lr\", \"early_stopping\"],         #\n",
    "                      \"patience_early_stopping\" : 20,                                                   #\n",
    "                      \"patience_reduce_lr\" : 10,                                                        #\n",
    "                      \"test_size\" : 0.2                                                                 #\n",
    "                     }                                                                                  #\n",
    "#SIN Reduce_lr en callbacks                                                                             #\n",
    "hiper_params_test12_20 = {\"class_weight\" : {0: 1, 1: 1},                                                 #\n",
    "                      \"batch_size\" : 32,                                                                #\n",
    "                      \"epochs\" : 160,                                                                   #\n",
    "                      \"define_callbacks\" : [\"early_stopping\"],                      #\n",
    "                      \"patience_early_stopping\" : 14,                                                   #\n",
    "                      \"test_size\" : 0.2                                                                 #\n",
    "                     }                                                                                  #\n",
    "#########################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Hiperparámetros para pruebas: aumento patience en early stopping = 20 y LR descendiente = 10, split de 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "###################################### CLASES BALANCEADAS ###############################################\n",
    "#########################################################################################################\n",
    "                                                                                                        #\n",
    "# Reduce_lr en callbacks                                                                                #\n",
    "hiper_params_test9_30 = {\"class_weight\" : class_weight_dict, #\n",
    "                      \"batch_size\" : 16,                                                                #\n",
    "                      \"epochs\" : 160,                                                                   #\n",
    "                      \"define_callbacks\" : [\"reduce_lr\", \"early_stopping\"],         #\n",
    "                      \"patience_early_stopping\" : 20,                                                   #\n",
    "                      \"patience_reduce_lr\" : 10,                                                        #\n",
    "                      \"test_size\" : 0.3                                                                 #\n",
    "                     }                                                                                  #         \n",
    "                                                                                                        #\n",
    "# SIN reduce_lr en callbacks                                                                            #   \n",
    "hiper_params_test10_30 = {\"class_weight\" : class_weight_dict, #\n",
    "                      \"batch_size\" : 16,                                                                #\n",
    "                      \"epochs\" : 160,                                                                   #\n",
    "                      \"define_callbacks\" : [\"early_stopping\"],                      #\n",
    "                      \"patience_early_stopping\" : 14,                                                   #\n",
    "                      \"test_size\" : 0.3                                                                 #\n",
    "                     }                                                                                  #\n",
    "#########################################################################################################\n",
    "\n",
    "######################################################################################################\n",
    "###################################### CLASES NO BALANCEADAS #########################################\n",
    "######################################################################################################\n",
    "                                                                                                     #\n",
    "# Reduce_lr en callbacks                                                                             #  \n",
    "hiper_params_test11_30 = {\"class_weight\" : {0: 1, 1: 1},                                              #\n",
    "                      \"batch_size\" : 16,                                                             #\n",
    "                      \"epochs\" : 160,                                                                #\n",
    "                      \"define_callbacks\" : [\"reduce_lr\", \"early_stopping\"],      #\n",
    "                      \"patience_early_stopping\" : 20,                                                #\n",
    "                      \"patience_reduce_lr\" : 10,                                                     #\n",
    "                      \"test_size\" : 0.3                                                              #\n",
    "                     }                                                                               #\n",
    "                                                                                                     #\n",
    "# SIN reduce_lr en callbacks                                                                         #      \n",
    "hiper_params_test12_30 = {\"class_weight\" : {0: 1, 1: 1},                                              #\n",
    "                      \"batch_size\" : 16,                                                             #\n",
    "                      \"epochs\" : 160,                                                                #\n",
    "                      \"define_callbacks\" : [\"early_stopping\"],                   #\n",
    "                      \"patience_early_stopping\" : 20,                                                #\n",
    "                      \"test_size\" : 0.3                                                              #\n",
    "                     }                                                                               #\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generamos una lista con todos los hiperparámetros definidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiperparametros_20 = [hiper_params_test1_20, hiper_params_test2_20, hiper_params_test3_20, hiper_params_test4_20]\n",
    "hiperparametros_30 = [hiper_params_test1_30, hiper_params_test2_30, hiper_params_test3_30, hiper_params_test4_30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiperparametros_20_modified = [hiper_params_test5_20, hiper_params_test6_20, hiper_params_test7_20, hiper_params_test8_20]\n",
    "hiperparametros_30_modified = [hiper_params_test5_30, hiper_params_test6_30, hiper_params_test7_30, hiper_params_test8_30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiperparametros_20_max_patience = [hiper_params_test9_20, hiper_params_test10_20, hiper_params_test11_20, hiper_params_test12_20]\n",
    "hiperparametros_30_max_patience = [hiper_params_test9_30, hiper_params_test10_30, hiper_params_test11_30, hiper_params_test12_30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de las diferentes arquitecturas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de modelos construidos = 6 pruebas * 4 hiperparámetros/prueba * 2 semillas distintas * 4 arquitecturas distintas = 192 modelos construidos. 48 modelos por cada arquitectura.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total de modelos construidos = 6 pruebas * 4 hiperparámetros/prueba * 2 semillas distintas * 4 arquitecturas distintas = {6*4*2*4} modelos construidos. 48 modelos por cada arquitectura.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    # [\"split_20\",hiperparametros_20],\n",
    "    # [\"split_30\",hiperparametros_30],\n",
    "    # [\"split_20_increase_patience\",hiperparametros_20_modified],\n",
    "    # [\"split_30_increase_patience\",hiperparametros_30_modified],\n",
    "    [\"split_20_max_patience\",hiperparametros_20_max_patience],\n",
    "    [\"split_30_max_patience\",hiperparametros_30_max_patience]\n",
    "]\n",
    "\n",
    "input_shape = (48,48,12)\n",
    "\n",
    "modelos = [\n",
    "    # \"alexnet\",\n",
    "    # \"scratch\",\n",
    "    # \"vgg\",\n",
    "    \"mobilenet\"\n",
    "]\n",
    "\n",
    "# first_top_model_layer = \"Flatten\"\n",
    "first_top_model_layer = \"GlobalMaxPooling\"\n",
    "# first_top_model_layer = \"GlobalAveragePooling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raulpg/anaconda3/envs/tfm_tf/lib/python3.11/site-packages/keras/src/legacy/preprocessing/image.py:1495: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (3275, 48, 48, 12) (12 channels).\n",
      "  warnings.warn(\n",
      "/home/raulpg/anaconda3/envs/tfm_tf/lib/python3.11/site-packages/keras/src/legacy/preprocessing/image.py:619: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (3275, 48, 48, 12) (12 channels).\n",
      "  warnings.warn(\n",
      "/home/raulpg/anaconda3/envs/tfm_tf/lib/python3.11/site-packages/keras/src/legacy/preprocessing/image.py:619: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (819, 48, 48, 12) (12 channels).\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/raulp/Documents/Master/TFM/minig-detector-tfm/scripts/models.py:160: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNet(weights='imagenet',include_top=False)\n",
      "2024-09-27 13:59:14.470186: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-27 13:59:14.552519: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-27 13:59:14.552580: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-27 13:59:14.555137: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-27 13:59:14.555194: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-27 13:59:14.555209: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-27 13:59:14.685153: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-27 13:59:14.685193: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-27 13:59:14.685197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-09-27 13:59:14.685209: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:236] Using CUDA malloc Async allocator for GPU: 0\n",
      "2024-09-27 13:59:14.685401: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-27 13:59:14.685428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5529 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2024-09-27 13:59:14.878260: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raulpg/anaconda3/envs/tfm_tf/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "2024-09-27 13:59:19.678057: I external/local_xla/xla/service/service.cc:168] XLA service 0xc270e20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-09-27 13:59:19.678088: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Ti, Compute Capability 8.9\n",
      "2024-09-27 13:59:19.783395: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-09-27 13:59:20.436490: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  7/103\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - acc: 0.5850 - loss: 1.1790"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727438367.885421  181408 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 27/103\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 291ms/step - acc: 0.6319 - loss: 0.9910"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1727438375.329871  181407 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 128ms/step - acc: 0.7049 - loss: 0.7568 - val_acc: 0.6129 - val_loss: 0.8502 - learning_rate: 3.0000e-04\n",
      "Epoch 2/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.8189 - loss: 0.4336 - val_acc: 0.6044 - val_loss: 2.6472 - learning_rate: 3.0000e-04\n",
      "Epoch 3/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.8481 - loss: 0.3526 - val_acc: 0.6044 - val_loss: 2.6092 - learning_rate: 3.0000e-04\n",
      "Epoch 4/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.8760 - loss: 0.3141 - val_acc: 0.6716 - val_loss: 1.0114 - learning_rate: 3.0000e-04\n",
      "Epoch 5/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.8885 - loss: 0.2646 - val_acc: 0.8217 - val_loss: 0.3673 - learning_rate: 3.0000e-04\n",
      "Epoch 6/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.8996 - loss: 0.2604 - val_acc: 0.8987 - val_loss: 0.2390 - learning_rate: 3.0000e-04\n",
      "Epoch 7/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9090 - loss: 0.2276 - val_acc: 0.9341 - val_loss: 0.1857 - learning_rate: 3.0000e-04\n",
      "Epoch 8/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - acc: 0.9144 - loss: 0.2075 - val_acc: 0.9158 - val_loss: 0.1900 - learning_rate: 3.0000e-04\n",
      "Epoch 9/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - acc: 0.9257 - loss: 0.1940 - val_acc: 0.8938 - val_loss: 0.2582 - learning_rate: 3.0000e-04\n",
      "Epoch 10/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - acc: 0.9259 - loss: 0.2185 - val_acc: 0.9206 - val_loss: 0.1872 - learning_rate: 3.0000e-04\n",
      "Epoch 11/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - acc: 0.9248 - loss: 0.1833 - val_acc: 0.9365 - val_loss: 0.1768 - learning_rate: 3.0000e-04\n",
      "Epoch 12/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9352 - loss: 0.1618 - val_acc: 0.9060 - val_loss: 0.2086 - learning_rate: 3.0000e-04\n",
      "Epoch 13/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9401 - loss: 0.1490 - val_acc: 0.9414 - val_loss: 0.1525 - learning_rate: 3.0000e-04\n",
      "Epoch 14/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9389 - loss: 0.1589 - val_acc: 0.9292 - val_loss: 0.1828 - learning_rate: 3.0000e-04\n",
      "Epoch 15/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - acc: 0.9439 - loss: 0.1467 - val_acc: 0.9377 - val_loss: 0.1786 - learning_rate: 3.0000e-04\n",
      "Epoch 16/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9377 - loss: 0.1594 - val_acc: 0.9084 - val_loss: 0.2195 - learning_rate: 3.0000e-04\n",
      "Epoch 17/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - acc: 0.9447 - loss: 0.1468 - val_acc: 0.9243 - val_loss: 0.1886 - learning_rate: 3.0000e-04\n",
      "Epoch 18/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9523 - loss: 0.1172 - val_acc: 0.9463 - val_loss: 0.1353 - learning_rate: 3.0000e-04\n",
      "Epoch 19/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.9538 - loss: 0.1246 - val_acc: 0.9023 - val_loss: 0.2972 - learning_rate: 3.0000e-04\n",
      "Epoch 20/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9514 - loss: 0.1278 - val_acc: 0.9536 - val_loss: 0.1127 - learning_rate: 3.0000e-04\n",
      "Epoch 21/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9596 - loss: 0.1089 - val_acc: 0.9280 - val_loss: 0.1771 - learning_rate: 3.0000e-04\n",
      "Epoch 22/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9629 - loss: 0.1028 - val_acc: 0.9292 - val_loss: 0.1806 - learning_rate: 3.0000e-04\n",
      "Epoch 23/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - acc: 0.9563 - loss: 0.1240 - val_acc: 0.9133 - val_loss: 0.2384 - learning_rate: 3.0000e-04\n",
      "Epoch 24/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9577 - loss: 0.1065 - val_acc: 0.9438 - val_loss: 0.1455 - learning_rate: 3.0000e-04\n",
      "Epoch 25/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9654 - loss: 0.0974 - val_acc: 0.9231 - val_loss: 0.1943 - learning_rate: 3.0000e-04\n",
      "Epoch 26/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - acc: 0.9594 - loss: 0.0924 - val_acc: 0.9255 - val_loss: 0.2048 - learning_rate: 3.0000e-04\n",
      "Epoch 27/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9609 - loss: 0.1019 - val_acc: 0.9512 - val_loss: 0.1344 - learning_rate: 3.0000e-04\n",
      "Epoch 28/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9625 - loss: 0.0917 - val_acc: 0.9267 - val_loss: 0.1882 - learning_rate: 3.0000e-04\n",
      "Epoch 29/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9694 - loss: 0.0801 - val_acc: 0.9292 - val_loss: 0.1675 - learning_rate: 3.0000e-04\n",
      "Epoch 30/160\n",
      "\u001b[1m101/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - acc: 0.9687 - loss: 0.0757\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.00022500001068692654.\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - acc: 0.9687 - loss: 0.0760 - val_acc: 0.9316 - val_loss: 0.1832 - learning_rate: 3.0000e-04\n",
      "Epoch 31/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - acc: 0.9721 - loss: 0.0753 - val_acc: 0.9536 - val_loss: 0.1257 - learning_rate: 2.2500e-04\n",
      "Epoch 32/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - acc: 0.9810 - loss: 0.0516 - val_acc: 0.9463 - val_loss: 0.1330 - learning_rate: 2.2500e-04\n",
      "Epoch 33/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9770 - loss: 0.0670 - val_acc: 0.9536 - val_loss: 0.1577 - learning_rate: 2.2500e-04\n",
      "Epoch 34/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - acc: 0.9772 - loss: 0.0590 - val_acc: 0.8962 - val_loss: 0.2441 - learning_rate: 2.2500e-04\n",
      "Epoch 35/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.9750 - loss: 0.0639 - val_acc: 0.9328 - val_loss: 0.1821 - learning_rate: 2.2500e-04\n",
      "Epoch 36/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.9775 - loss: 0.0521 - val_acc: 0.9573 - val_loss: 0.1119 - learning_rate: 2.2500e-04\n",
      "Epoch 37/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.9817 - loss: 0.0446 - val_acc: 0.9463 - val_loss: 0.1412 - learning_rate: 2.2500e-04\n",
      "Epoch 38/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.9764 - loss: 0.0624 - val_acc: 0.9389 - val_loss: 0.1800 - learning_rate: 2.2500e-04\n",
      "Epoch 39/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.9743 - loss: 0.0752 - val_acc: 0.9451 - val_loss: 0.1598 - learning_rate: 2.2500e-04\n",
      "Epoch 40/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.9851 - loss: 0.0429 - val_acc: 0.9487 - val_loss: 0.1545 - learning_rate: 2.2500e-04\n",
      "Epoch 41/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9808 - loss: 0.0486 - val_acc: 0.9585 - val_loss: 0.1370 - learning_rate: 2.2500e-04\n",
      "Epoch 42/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9834 - loss: 0.0430 - val_acc: 0.9353 - val_loss: 0.1639 - learning_rate: 2.2500e-04\n",
      "Epoch 43/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - acc: 0.9878 - loss: 0.0361 - val_acc: 0.9267 - val_loss: 0.2107 - learning_rate: 2.2500e-04\n",
      "Epoch 44/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9811 - loss: 0.0487 - val_acc: 0.9438 - val_loss: 0.2025 - learning_rate: 2.2500e-04\n",
      "Epoch 45/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9801 - loss: 0.0604 - val_acc: 0.9316 - val_loss: 0.2039 - learning_rate: 2.2500e-04\n",
      "Epoch 46/160\n",
      "\u001b[1m101/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - acc: 0.9826 - loss: 0.0478\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0001687500080151949.\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9826 - loss: 0.0478 - val_acc: 0.9475 - val_loss: 0.1460 - learning_rate: 2.2500e-04\n",
      "Epoch 47/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9876 - loss: 0.0293 - val_acc: 0.9585 - val_loss: 0.1149 - learning_rate: 1.6875e-04\n",
      "Epoch 48/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9876 - loss: 0.0375 - val_acc: 0.9695 - val_loss: 0.0860 - learning_rate: 1.6875e-04\n",
      "Epoch 49/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.9909 - loss: 0.0247 - val_acc: 0.9487 - val_loss: 0.2133 - learning_rate: 1.6875e-04\n",
      "Epoch 50/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.9894 - loss: 0.0296 - val_acc: 0.9560 - val_loss: 0.1509 - learning_rate: 1.6875e-04\n",
      "Epoch 51/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.9887 - loss: 0.0358 - val_acc: 0.9487 - val_loss: 0.1612 - learning_rate: 1.6875e-04\n",
      "Epoch 52/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.9873 - loss: 0.0426 - val_acc: 0.9463 - val_loss: 0.1754 - learning_rate: 1.6875e-04\n",
      "Epoch 53/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9887 - loss: 0.0292 - val_acc: 0.9670 - val_loss: 0.1050 - learning_rate: 1.6875e-04\n",
      "Epoch 54/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9887 - loss: 0.0367 - val_acc: 0.9597 - val_loss: 0.1354 - learning_rate: 1.6875e-04\n",
      "Epoch 55/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.9947 - loss: 0.0197 - val_acc: 0.9438 - val_loss: 0.2088 - learning_rate: 1.6875e-04\n",
      "Epoch 56/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9922 - loss: 0.0293 - val_acc: 0.9695 - val_loss: 0.1352 - learning_rate: 1.6875e-04\n",
      "Epoch 57/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9872 - loss: 0.0315 - val_acc: 0.9719 - val_loss: 0.1465 - learning_rate: 1.6875e-04\n",
      "Epoch 58/160\n",
      "\u001b[1m101/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - acc: 0.9922 - loss: 0.0251\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 0.00012656250328291208.\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9921 - loss: 0.0252 - val_acc: 0.9597 - val_loss: 0.1981 - learning_rate: 1.6875e-04\n",
      "Epoch 59/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9924 - loss: 0.0276 - val_acc: 0.9695 - val_loss: 0.1099 - learning_rate: 1.2656e-04\n",
      "Epoch 60/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9900 - loss: 0.0274 - val_acc: 0.9744 - val_loss: 0.1047 - learning_rate: 1.2656e-04\n",
      "Epoch 61/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9915 - loss: 0.0324 - val_acc: 0.9573 - val_loss: 0.1921 - learning_rate: 1.2656e-04\n",
      "Epoch 62/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9905 - loss: 0.0261 - val_acc: 0.9573 - val_loss: 0.1474 - learning_rate: 1.2656e-04\n",
      "Epoch 63/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9929 - loss: 0.0231 - val_acc: 0.9646 - val_loss: 0.1184 - learning_rate: 1.2656e-04\n",
      "Epoch 64/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9938 - loss: 0.0176 - val_acc: 0.9731 - val_loss: 0.1134 - learning_rate: 1.2656e-04\n",
      "Epoch 65/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9946 - loss: 0.0154 - val_acc: 0.9707 - val_loss: 0.1179 - learning_rate: 1.2656e-04\n",
      "Epoch 66/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9923 - loss: 0.0201 - val_acc: 0.9402 - val_loss: 0.2744 - learning_rate: 1.2656e-04\n",
      "Epoch 67/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.9934 - loss: 0.0223 - val_acc: 0.9536 - val_loss: 0.1735 - learning_rate: 1.2656e-04\n",
      "Epoch 68/160\n",
      "\u001b[1m 99/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - acc: 0.9951 - loss: 0.0150\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 9.492187746218406e-05.\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.9951 - loss: 0.0153 - val_acc: 0.9719 - val_loss: 0.1240 - learning_rate: 1.2656e-04\n",
      "Epoch 68: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raulpg/anaconda3/envs/tfm_tf/lib/python3.11/site-packages/keras/src/legacy/preprocessing/image.py:1495: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (3275, 48, 48, 12) (12 channels).\n",
      "  warnings.warn(\n",
      "/home/raulpg/anaconda3/envs/tfm_tf/lib/python3.11/site-packages/keras/src/legacy/preprocessing/image.py:619: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (3275, 48, 48, 12) (12 channels).\n",
      "  warnings.warn(\n",
      "/home/raulpg/anaconda3/envs/tfm_tf/lib/python3.11/site-packages/keras/src/legacy/preprocessing/image.py:619: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (819, 48, 48, 12) (12 channels).\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/raulp/Documents/Master/TFM/minig-detector-tfm/scripts/models.py:160: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNet(weights='imagenet',include_top=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raulpg/anaconda3/envs/tfm_tf/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 75/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - acc: 0.6982 - loss: 0.9192"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1727438628.888409  181408 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 67ms/step - acc: 0.7143 - loss: 0.8503 - val_acc: 0.6044 - val_loss: 0.9571 - learning_rate: 3.0000e-04\n",
      "Epoch 2/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.8145 - loss: 0.4303 - val_acc: 0.6166 - val_loss: 1.0213 - learning_rate: 3.0000e-04\n",
      "Epoch 3/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.8630 - loss: 0.3329 - val_acc: 0.6129 - val_loss: 1.3676 - learning_rate: 3.0000e-04\n",
      "Epoch 4/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.8545 - loss: 0.3317 - val_acc: 0.7705 - val_loss: 0.5925 - learning_rate: 3.0000e-04\n",
      "Epoch 5/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.8852 - loss: 0.2796 - val_acc: 0.7961 - val_loss: 0.4797 - learning_rate: 3.0000e-04\n",
      "Epoch 6/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.8959 - loss: 0.2487 - val_acc: 0.8828 - val_loss: 0.2522 - learning_rate: 3.0000e-04\n",
      "Epoch 7/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.8934 - loss: 0.2547 - val_acc: 0.9280 - val_loss: 0.1817 - learning_rate: 3.0000e-04\n",
      "Epoch 8/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9090 - loss: 0.2294 - val_acc: 0.8730 - val_loss: 0.2881 - learning_rate: 3.0000e-04\n",
      "Epoch 9/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - acc: 0.9126 - loss: 0.2138 - val_acc: 0.9219 - val_loss: 0.2023 - learning_rate: 3.0000e-04\n",
      "Epoch 10/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9235 - loss: 0.1944 - val_acc: 0.9255 - val_loss: 0.1898 - learning_rate: 3.0000e-04\n",
      "Epoch 11/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9289 - loss: 0.1715 - val_acc: 0.9206 - val_loss: 0.2017 - learning_rate: 3.0000e-04\n",
      "Epoch 12/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9259 - loss: 0.1800 - val_acc: 0.7985 - val_loss: 0.4723 - learning_rate: 3.0000e-04\n",
      "Epoch 13/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9394 - loss: 0.1542 - val_acc: 0.8828 - val_loss: 0.2619 - learning_rate: 3.0000e-04\n",
      "Epoch 14/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9302 - loss: 0.1714 - val_acc: 0.8950 - val_loss: 0.2409 - learning_rate: 3.0000e-04\n",
      "Epoch 15/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9313 - loss: 0.1593 - val_acc: 0.9475 - val_loss: 0.1536 - learning_rate: 3.0000e-04\n",
      "Epoch 16/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9455 - loss: 0.1393 - val_acc: 0.9292 - val_loss: 0.2004 - learning_rate: 3.0000e-04\n",
      "Epoch 17/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - acc: 0.9397 - loss: 0.1455 - val_acc: 0.8694 - val_loss: 0.2894 - learning_rate: 3.0000e-04\n",
      "Epoch 18/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9434 - loss: 0.1413 - val_acc: 0.9231 - val_loss: 0.1815 - learning_rate: 3.0000e-04\n",
      "Epoch 19/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9532 - loss: 0.1282 - val_acc: 0.9219 - val_loss: 0.1855 - learning_rate: 3.0000e-04\n",
      "Epoch 20/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9437 - loss: 0.1363 - val_acc: 0.9194 - val_loss: 0.2210 - learning_rate: 3.0000e-04\n",
      "Epoch 21/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9563 - loss: 0.1176 - val_acc: 0.9475 - val_loss: 0.1679 - learning_rate: 3.0000e-04\n",
      "Epoch 22/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - acc: 0.9623 - loss: 0.1035 - val_acc: 0.9328 - val_loss: 0.1728 - learning_rate: 3.0000e-04\n",
      "Epoch 23/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9596 - loss: 0.1201 - val_acc: 0.9353 - val_loss: 0.1761 - learning_rate: 3.0000e-04\n",
      "Epoch 24/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - acc: 0.9563 - loss: 0.1030 - val_acc: 0.9267 - val_loss: 0.2000 - learning_rate: 3.0000e-04\n",
      "Epoch 25/160\n",
      "\u001b[1m101/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - acc: 0.9571 - loss: 0.1093\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.00022500001068692654.\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - acc: 0.9571 - loss: 0.1093 - val_acc: 0.9499 - val_loss: 0.1564 - learning_rate: 3.0000e-04\n",
      "Epoch 26/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9562 - loss: 0.1052 - val_acc: 0.9499 - val_loss: 0.1615 - learning_rate: 2.2500e-04\n",
      "Epoch 27/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - acc: 0.9680 - loss: 0.0790 - val_acc: 0.9573 - val_loss: 0.1226 - learning_rate: 2.2500e-04\n",
      "Epoch 28/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - acc: 0.9621 - loss: 0.0917 - val_acc: 0.9267 - val_loss: 0.2355 - learning_rate: 2.2500e-04\n",
      "Epoch 29/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.9649 - loss: 0.0862 - val_acc: 0.9365 - val_loss: 0.1522 - learning_rate: 2.2500e-04\n",
      "Epoch 30/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.9630 - loss: 0.0866 - val_acc: 0.9292 - val_loss: 0.2445 - learning_rate: 2.2500e-04\n",
      "Epoch 31/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.9734 - loss: 0.0694 - val_acc: 0.9145 - val_loss: 0.2808 - learning_rate: 2.2500e-04\n",
      "Epoch 32/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.9704 - loss: 0.0813 - val_acc: 0.9365 - val_loss: 0.1817 - learning_rate: 2.2500e-04\n",
      "Epoch 33/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.9740 - loss: 0.0675 - val_acc: 0.9548 - val_loss: 0.1259 - learning_rate: 2.2500e-04\n",
      "Epoch 34/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.9770 - loss: 0.0687 - val_acc: 0.9609 - val_loss: 0.1210 - learning_rate: 2.2500e-04\n",
      "Epoch 35/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.9721 - loss: 0.0669 - val_acc: 0.9182 - val_loss: 0.2038 - learning_rate: 2.2500e-04\n",
      "Epoch 36/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.9756 - loss: 0.0656 - val_acc: 0.9524 - val_loss: 0.1171 - learning_rate: 2.2500e-04\n",
      "Epoch 37/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - acc: 0.9809 - loss: 0.0573 - val_acc: 0.9499 - val_loss: 0.1558 - learning_rate: 2.2500e-04\n",
      "Epoch 38/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9810 - loss: 0.0462 - val_acc: 0.9499 - val_loss: 0.1708 - learning_rate: 2.2500e-04\n",
      "Epoch 39/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9797 - loss: 0.0608 - val_acc: 0.9328 - val_loss: 0.1702 - learning_rate: 2.2500e-04\n",
      "Epoch 40/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9795 - loss: 0.0508 - val_acc: 0.9353 - val_loss: 0.1917 - learning_rate: 2.2500e-04\n",
      "Epoch 41/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9739 - loss: 0.0644 - val_acc: 0.9414 - val_loss: 0.1581 - learning_rate: 2.2500e-04\n",
      "Epoch 42/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9808 - loss: 0.0506 - val_acc: 0.9512 - val_loss: 0.1307 - learning_rate: 2.2500e-04\n",
      "Epoch 43/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9804 - loss: 0.0509 - val_acc: 0.9560 - val_loss: 0.1269 - learning_rate: 2.2500e-04\n",
      "Epoch 44/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - acc: 0.9796 - loss: 0.0504 - val_acc: 0.9389 - val_loss: 0.1863 - learning_rate: 2.2500e-04\n",
      "Epoch 45/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9731 - loss: 0.0590 - val_acc: 0.9328 - val_loss: 0.2563 - learning_rate: 2.2500e-04\n",
      "Epoch 46/160\n",
      "\u001b[1m101/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - acc: 0.9830 - loss: 0.0472\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0001687500080151949.\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - acc: 0.9829 - loss: 0.0474 - val_acc: 0.9426 - val_loss: 0.1582 - learning_rate: 2.2500e-04\n",
      "Epoch 47/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9836 - loss: 0.0436 - val_acc: 0.9426 - val_loss: 0.2099 - learning_rate: 1.6875e-04\n",
      "Epoch 48/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9853 - loss: 0.0444 - val_acc: 0.9683 - val_loss: 0.1258 - learning_rate: 1.6875e-04\n",
      "Epoch 49/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9868 - loss: 0.0284 - val_acc: 0.9573 - val_loss: 0.1271 - learning_rate: 1.6875e-04\n",
      "Epoch 50/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9885 - loss: 0.0314 - val_acc: 0.9487 - val_loss: 0.1820 - learning_rate: 1.6875e-04\n",
      "Epoch 51/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9866 - loss: 0.0293 - val_acc: 0.9512 - val_loss: 0.2127 - learning_rate: 1.6875e-04\n",
      "Epoch 52/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9910 - loss: 0.0309 - val_acc: 0.9585 - val_loss: 0.1663 - learning_rate: 1.6875e-04\n",
      "Epoch 53/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9891 - loss: 0.0257 - val_acc: 0.9499 - val_loss: 0.1451 - learning_rate: 1.6875e-04\n",
      "Epoch 54/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9904 - loss: 0.0267 - val_acc: 0.9573 - val_loss: 0.1767 - learning_rate: 1.6875e-04\n",
      "Epoch 55/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.9889 - loss: 0.0313 - val_acc: 0.9341 - val_loss: 0.2823 - learning_rate: 1.6875e-04\n",
      "Epoch 56/160\n",
      "\u001b[1m101/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - acc: 0.9843 - loss: 0.0332\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 0.00012656250328291208.\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - acc: 0.9843 - loss: 0.0334 - val_acc: 0.9573 - val_loss: 0.1359 - learning_rate: 1.6875e-04\n",
      "Epoch 56: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/raulp/Documents/Master/TFM/minig-detector-tfm/scripts/dl_utils.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  valores.append(float(df.iloc[3][-2]))\n",
      "/mnt/c/Users/raulp/Documents/Master/TFM/minig-detector-tfm/scripts/dl_utils.py:176: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  valores_old.append(float(prev_df.iloc[3][-2]))\n",
      "/home/raulpg/anaconda3/envs/tfm_tf/lib/python3.11/site-packages/keras/src/legacy/preprocessing/image.py:1495: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (3275, 48, 48, 12) (12 channels).\n",
      "  warnings.warn(\n",
      "/home/raulpg/anaconda3/envs/tfm_tf/lib/python3.11/site-packages/keras/src/legacy/preprocessing/image.py:619: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (3275, 48, 48, 12) (12 channels).\n",
      "  warnings.warn(\n",
      "/home/raulpg/anaconda3/envs/tfm_tf/lib/python3.11/site-packages/keras/src/legacy/preprocessing/image.py:619: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (819, 48, 48, 12) (12 channels).\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/raulp/Documents/Master/TFM/minig-detector-tfm/scripts/models.py:160: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNet(weights='imagenet',include_top=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raulpg/anaconda3/envs/tfm_tf/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m102/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - acc: 0.7060 - loss: 0.7702"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1727438839.300170  181409 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 68ms/step - acc: 0.7071 - loss: 0.7672 - val_acc: 0.6044 - val_loss: 2.1403\n",
      "Epoch 2/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - acc: 0.8211 - loss: 0.4251 - val_acc: 0.6044 - val_loss: 1.6389\n",
      "Epoch 3/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.8369 - loss: 0.3791 - val_acc: 0.6166 - val_loss: 1.3065\n",
      "Epoch 4/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - acc: 0.8802 - loss: 0.2982 - val_acc: 0.7851 - val_loss: 0.5473\n",
      "Epoch 5/160\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - acc: 0.8985 - loss: 0.2626 - val_acc: 0.8303 - val_loss: 0.3546\n",
      "Epoch 6/160\n",
      "\u001b[1m 21/103\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - acc: 0.9038 - loss: 0.2625"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:135\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/tfm_tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfm_tf/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    322\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfm_tf/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfm_tf/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfm_tf/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:877\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 877\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    881\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    882\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfm_tf/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tfm_tf/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/tfm_tf/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfm_tf/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/tfm_tf/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/envs/tfm_tf/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for test in tests:\n",
    "\n",
    "    # Descripcion de esta prueba\n",
    "    desc_test = test[0]\n",
    "    hiperparametros = test[1]\n",
    "    \n",
    "    # Inicializamos para dar nombre a todos los outputs que se escriban\n",
    "    hora_exec = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    \n",
    "    # Creamos el nombre del directorio para guardar los modelos finales\n",
    "    path_prueba_actual = f\"../notebooks/best_models/{desc_test}_{hora_exec}/\"\n",
    "    os.makedirs(path_prueba_actual, exist_ok=True)\n",
    "    \n",
    "    # Inicializamos las variables dónde almacenaremos los resultados de cada uno de los modelos generados\n",
    "    df_alexnet = pd.DataFrame()\n",
    "    df_scratch_modified = pd.DataFrame()\n",
    "    df_vgg = pd.DataFrame()\n",
    "    df_mobile_net = pd.DataFrame()\n",
    "    \n",
    "    #Eliminamos los csvs generados en ejecuciones anteriores\n",
    "    dl_utils.limpia_directorios(\"aux_path\")\n",
    "    \n",
    "    # Recorremos la lista de configuraciones distinta generada anteriormente\n",
    "    for id_hiperparams,hiperparams in enumerate(hiperparametros):\n",
    "        for semilla in range(0,2):  \n",
    "    \n",
    "            # Limpiamos la carpeta donde se guardan los modelos\n",
    "            dl_utils.limpia_directorios(\"weights\")\n",
    "    \n",
    "            # Definimos hiperparámetros asociados a la ejecución actual\n",
    "            batch_size = hiperparams[\"batch_size\"]\n",
    "            epochs = hiperparams[\"epochs\"]\n",
    "         \n",
    "            # Dividimos el conjunto de imágenes en train y test\n",
    "            seed = np.random.randint(0, 2000,1)[0]\n",
    "            hiperparams[\"semilla\"] = seed\n",
    "            keras.utils.set_random_seed(int(seed))\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x_norm, y, test_size=hiperparams[\"test_size\"], random_state=123)\n",
    "    \n",
    "            # Generamos los datagen de imágenes, empleamos las mismas imágenes en todos los modelos\n",
    "            datagen.fit(x_train)\n",
    "            train_generator = datagen.flow(x_train, y_train, batch_size=batch_size, shuffle=False)\n",
    "            test_generator = datagen.flow(x_test, y_test, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "############################################################################### ALEXNET ##########################################################################################################\n",
    "            if \"alexnet\" in modelos:\n",
    "                # ALEXNET\n",
    "                # Definimos y compilamos los modelos\n",
    "                alexnet = models.alexnet_model(input_shape)\n",
    "                start_time = time.time()\n",
    "                alexnet_results = alexnet.fit(\n",
    "                    train_generator,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=test_generator,\n",
    "                    verbose=1,\n",
    "                    shuffle=True,\n",
    "                    callbacks=dl_utils.define_callbacks(\"model_alexnet_\",hiperparams),\n",
    "                    class_weight=hiperparams[\"class_weight\"],\n",
    "                )\n",
    "                training_time_alexnet = time.time() - start_time\n",
    "                hora_actual = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "                # Evaluamos el modelo y lo comparamos con el mejor almacenado.\n",
    "                df_alexnet,           save_df_alexnet         = dl_utils.get_df_best_model(alexnet,test_partition_x,test_partition_y,f\"alexnet_{desc_test}_\",df_alexnet,hora_actual)\n",
    "                if save_df_alexnet:\n",
    "                    best_alexnet_model = alexnet\n",
    "                    best_alexnet_results = alexnet_results.history\n",
    "                    best_alexnet_hiperparams = [hiperparams,\n",
    "                                                id_hiperparams+1,\n",
    "                                                f'resultados_evaluacion_alexnet_{desc_test}_{hora_actual}.csv',\n",
    "                                                training_time_alexnet,\n",
    "                                                len(alexnet_results.epoch)]           \n",
    "                    \n",
    "############################################################################### SCRATCH MODIFIED ##########################################################################################################            \n",
    "            if \"scratch\" in modelos:\n",
    "                scratch_modified_model = models.scratch_modified_model(input_shape)          \n",
    "                # Modelo from scratch modificado, ampliación del modelo original\n",
    "                start_time = time.time()\n",
    "                scratch_modified_model_results = scratch_modified_model.fit(\n",
    "                    train_generator,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=test_generator,\n",
    "                    verbose=1,\n",
    "                    shuffle=True,\n",
    "                    callbacks=dl_utils.define_callbacks(\"model_scratch_modified_\",hiperparams),\n",
    "                    class_weight=hiperparams[\"class_weight\"],\n",
    "                )\n",
    "                training_time_scratch_modified_model = time.time() - start_time\n",
    "                hora_actual = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "                # Evaluamos el modelo y lo comparamos con el mejor almacenado.\n",
    "                df_scratch_modified, save_df_scratch_modified = dl_utils.get_df_best_model(scratch_modified_model,test_partition_x,test_partition_y,f\"scratch_modified_model_{desc_test}_\",df_scratch_modified,hora_actual)\n",
    "                if save_df_scratch_modified:\n",
    "                    best_scratch_modified_model = scratch_modified_model\n",
    "                    best_scratch_modified_model_results = scratch_modified_model_results.history\n",
    "                    best_scratch_modified_hiperparams = [hiperparams,\n",
    "                                                         id_hiperparams+1,\n",
    "                                                         f'resultados_evaluacion_scratch_modified_model_{desc_test}_{hora_actual}.csv',\n",
    "                                                         training_time_scratch_modified_model,\n",
    "                                                         len(scratch_modified_model_results.epoch)]\n",
    "                    \n",
    "############################################################################### VGG16 ##########################################################################################################\n",
    "            if \"vgg\" in modelos:\n",
    "                vgg16 = models.vgg_16_model(input_shape,first_top_model_layer)\n",
    "                # Fine-tunning con los pesos del modelo VGG16\n",
    "                start_time = time.time()\n",
    "                vgg16_results = vgg16.fit(\n",
    "                    train_generator,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=test_generator,\n",
    "                    verbose=1,\n",
    "                    shuffle=True,\n",
    "                    callbacks=dl_utils.define_callbacks(\"model_vgg16_\",hiperparams),\n",
    "                    class_weight=hiperparams[\"class_weight\"],\n",
    "                )\n",
    "                training_time_vgg16 = time.time() - start_time\n",
    "                hora_actual = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "                # Evaluamos el modelo y lo comparamos con el mejor almacenado.\n",
    "                df_vgg,              save_df_vgg              = dl_utils.get_df_best_model(vgg16,test_partition_x,test_partition_y,f\"vgg16_{desc_test}_\",df_vgg,hora_actual)\n",
    "                if save_df_vgg:\n",
    "                    best_vgg_model = vgg16\n",
    "                    best_vgg_model_results = vgg16_results.history\n",
    "                    best_vgg_hiperparams =[hiperparams,\n",
    "                                           id_hiperparams+1,\n",
    "                                           f'resultados_evaluacion_vgg16_{desc_test}_{hora_actual}.csv',\n",
    "                                           training_time_vgg16,\n",
    "                                           len(vgg16_results.epoch)]\n",
    "                    \n",
    "############################################################################### MOBILENET ##########################################################################################################\n",
    "            if \"mobilenet\" in modelos:\n",
    "                mobile_net = models.mobilenet_model(input_shape,first_top_model_layer)\n",
    "                # Fine-tunning con los pesos del modelo MobileNet\n",
    "                start_time = time.time()\n",
    "                mobile_net_results = mobile_net.fit(\n",
    "                    train_generator,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=test_generator,\n",
    "                    verbose=1,\n",
    "                    shuffle=True,\n",
    "                    callbacks=dl_utils.define_callbacks(\"model_mobilenet_\",hiperparams),\n",
    "                    class_weight=hiperparams[\"class_weight\"],\n",
    "                )\n",
    "                training_time_mobile_net = time.time() - start_time\n",
    "                hora_actual = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "                # Evaluamos el modelo y lo comparamos con el mejor almacenado.\n",
    "                df_mobile_net,       save_df_mobile_net       = dl_utils.get_df_best_model(mobile_net,test_partition_x,test_partition_y,f\"mobile_net_{desc_test}_\",df_mobile_net,hora_actual)\n",
    "                # Si el modelo actual es mejor que el almacenado, guardamos su información.      \n",
    "                if save_df_mobile_net:\n",
    "                    best_mobile_net_model = mobile_net\n",
    "                    best_mobile_net_model_results = mobile_net_results.history\n",
    "                    best_mobile_net_hiperparams = [hiperparams,\n",
    "                                                   id_hiperparams+1,\n",
    "                                                   f'resultados_evaluacion_mobile_net_{desc_test}_{hora_actual}.csv',\n",
    "                                                   training_time_mobile_net,\n",
    "                                                   len(mobile_net_results.epoch)]\n",
    "                    \n",
    "############################################################################################################################################################################\n",
    "            # Limpiamos variables\n",
    "            if \"alexnet\" in modelos:\n",
    "                del alexnet\n",
    "                del alexnet_results\n",
    "            if \"scratch\" in modelos:\n",
    "                del scratch_modified_model\n",
    "                del scratch_modified_model_results\n",
    "            if \"vgg\" in modelos:\n",
    "                del vgg16\n",
    "                del vgg16_results\n",
    "            if \"mobilenet\" in modelos:\n",
    "                del mobile_net\n",
    "                del mobile_net_results\n",
    "            del train_generator\n",
    "            del test_generator\n",
    "            K.clear_session()\n",
    "            gc.collect()        \n",
    "\n",
    "    if \"alexnet\" in modelos:\n",
    "        dl_utils.save_models(best_alexnet_model,\n",
    "                             best_alexnet_results,\n",
    "                             best_alexnet_hiperparams,\n",
    "                             hora_exec,\n",
    "                             f'alexnet_{desc_test}',\n",
    "                             path_prueba_actual)\n",
    "    if \"scratch\" in modelos:\n",
    "        dl_utils.save_models(best_scratch_modified_model,\n",
    "                             best_scratch_modified_model_results,\n",
    "                             best_scratch_modified_hiperparams,\n",
    "                             hora_exec,\n",
    "                             f'scratch_modified_model_{desc_test}',\n",
    "                             path_prueba_actual)\n",
    "    if \"vgg\" in modelos:\n",
    "        dl_utils.save_models(best_vgg_model,\n",
    "                         best_vgg_model_results,\n",
    "                         best_vgg_hiperparams,\n",
    "                         hora_exec,\n",
    "                         f'vgg16_{desc_test}',\n",
    "                         path_prueba_actual)\n",
    "    if \"mobilenet\" in modelos:\n",
    "        dl_utils.save_models(best_mobile_net_model,\n",
    "                         best_mobile_net_model_results,\n",
    "                         best_mobile_net_hiperparams,\n",
    "                         hora_exec,\n",
    "                         f'mobile_net_{desc_test}',\n",
    "                         path_prueba_actual)\n",
    "  \n",
    "    # Movemos los archivos asociados a los resultados\n",
    "    paths_to_move = []\n",
    "    if \"alexnet\" in modelos:\n",
    "        paths_to_move.append(best_alexnet_hiperparams[2])\n",
    "    if \"scratch\" in modelos:\n",
    "        paths_to_move.append(best_scratch_modified_hiperparams[2])\n",
    "    if \"vgg\" in modelos:\n",
    "        paths_to_move.append(best_vgg_hiperparams[2])\n",
    "    if \"mobilenet\" in modelos:\n",
    "        paths_to_move.append(best_mobile_net_hiperparams[2])\n",
    "        \n",
    "    dl_utils.obtener_tablas_resultado(path_prueba_actual,paths_to_move)\n",
    "    \n",
    "    # Limpiamos variables auxiliares\n",
    "    if \"alexnet\" in modelos:\n",
    "        del best_alexnet_model\n",
    "        del best_alexnet_results\n",
    "        del best_alexnet_hiperparams\n",
    "    if \"scratch\" in modelos:    \n",
    "        del best_scratch_modified_model\n",
    "        del best_scratch_modified_model_results\n",
    "        del best_scratch_modified_hiperparams\n",
    "    if \"vgg\" in modelos:    \n",
    "        del best_vgg_model\n",
    "        del best_vgg_model_results\n",
    "        del best_vgg_hiperparams\n",
    "    if \"mobilenet\" in modelos:    \n",
    "        del best_mobile_net_model\n",
    "        del best_mobile_net_model_results\n",
    "        del best_mobile_net_hiperparams\n",
    "    K.clear_session()\n",
    "    gc.collect() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "bec97cbb607180795486aa419a93884fe3d0b55501c3e5098d64200fe61c3ffb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
